# Median Computation

This repository contains a set of programs that extract data from csv files and compute the average, median, and name of entity with median age across all the files provided. The expected file format is a sequence of lines each containing 3 data points consisting of a first name, a last name, and an age. There are three main executables: qs_median (uses QuickSelect to determine median), heap_median (uses a pair of min/max heaps to determine median), and stat_median (uses Python's statistics library to determine median). All of these files can be executed as follows

```
~$ *_median.py file1.csv file2.csv ...
```

**Note**: All implementations were developed and tested with Python 3.8. Any Python version > 3 should be sufficient, although this program is not compatible with Python 2.\*.

## Summary
A robust file and line processing procedure was developed and implemented in conjunction with three different algorithmic approaches to retrieving the median: median maintenance with a pair of min/max heaps, QuickSelect with a randomized pivot, and TimSort via the Python standard library's statistics module. On top of testing with the provided example files, three additional test files were generated with 100000, 1000000, 10000000 lines and used to assess the relative performance of each method (stats generated by cProfile).

Results, available in the statistics section, indicate that the TimSort method (using Pythons statistics library) is the most efficient, especially as the size of the input scales. However, the heap based method performed comparably and would be a better option if the data stream is constant and the median statistic is needed at multiple times throughout the process (as access to the median data at any given point is O(1)).


## Implementation

The final deliverable consists of three solutions (differing only in their algorithmic approach) that read data from multiple csv files (passed as command line arguments) and provide the average age, median age, and name of the entity with the median age (if one exists in the provided data). If no entity with the median age exists, the median age is reported along with a message indicating that no entity with that age was present in the data set. Additionally, the heap implementation takes the extra step of reporting the ages and names of the two entities that were used to compute the average median (the two entities with ages directly above and below the median).

The processing of the user file is robust. The program is able to detect and recover from system errors associated with non-existent urls or other file I/O issues. If a file contains malformed data (not csv, no header line), the file is reported as improperly formatted and skipped. Additionally, the lines within each csv are processed dynamically, in the sense that each file can contain a different ordering of the (fname, lname, and age) as long as that ordering is consistent throughout the file and defined in the first line of the file (header line). Each line is matched with a dynamically generated regex prior to processing and if an improperly structured line is detected the file and line number of the anomaly is reported and the malformed line is skipped.

The regular expression is based on the header line. For instance, if the header is age, lname, fname the regex will match 1-3 digits, a comma, 2-20 characters, a comma, then another 2-20 characters. In this way only lines that contain proper input are processed.

## Correctness

A tests.py file is included to demonstrate the correctness of each implementation. The tests are conducted by computing the median and average of the provided data using Python's statistics library then comparing the results with the values returned by each respective method (QuickSort, heap, and TimSort). This file can be executed in the same way as the as the others

```
~$ python3 tests.py file1.csv file2.csv ...
```

Several files with alternative orderings (fname, age, lname) and (age, fname, lname) are included in this repo in the TestFiles directory. Testing with these files demonstrates the dynamic processing described above. Additionally, GeneratedTestFiles.zip contains three test files generated by gentestfile.py (100000, 1000000, and 10000000 lines) which can be used to verify the relative profiling results described in the statistics section.

## Complexity
*Heap Median - O(n log(n))*
Each insertion requires O(log(n)) time, and there are at most n insertions. However, there is a tighter bound. Considering that the heap will grow logarithmically and initially the height will be 0, the only call that truly takes log(n) would be the final call. The height of heap will grow slowly in between those extremes.

Considering the bound actually is lower than O(n log(n)) and heap median provides access to the median at any point in execution in O(1) time, it is certainly a good candidate for this problem. It would be the most effective approach if the stream of data was constant and the median was needed many times throughout the process.

*TimSort - O(n log(n)) - omega(n)*
Python's statistics library uses a very efficient algorithm, TimSort, to sort the data prior to selecting the median. This approach requires a slightly elevated amount of memory, a dictionary correlating age to a list of names must be maintained as well as a list of all ages to compute the median of. Nonetheless, profiling (results available below in the statistics section) showed that this algorithm is the most efficient, surpassing the heap algorithm by 28%. This approach is the most effective for median computations involving a large data set in which the median is needed only once.

*QuickSelect - O(n^2) - omega(n)*
The final implementation uses QuickSelect with a randomized pivot. This approach performed comparably to heap median, however without the additional benefit of constant time access to the median at any point. Despite the average O(n) execution time, QuickSelect still lagged behind TimSort, and as such offers no benefits over the other two approaches.

However, it is interesting to note that QuickSort is particularly well suited for this problem, especially as the input size scales, because of the constraints of the numerical data (1-~110). In great quantity there will be many duplicate values. During the partitioning step, instead of determining if the pivot element is the desired element, a range of elements equal to the pivot element can be returned (all of which could effectively be in the place of the pivot).

For instance if the desired element has rank 50 in a list of 100 ages and after partitioning the pivot index is 43 but all elements spanning 40-60 are equivalent, then 43 is effectively rank 50 and can be returned. This results in a significant improvement in the overall performance of the QuickSelect approach, especially at scale.

### Statistics
Metrics below were generated with cProfile. Test files were generated with gentestfile.py (requires the Faker library).

**TimSort (Python Statistics Library)**

All provided test files (including edge cases):
137242 function calls (137032 primitive calls) in 0.072 seconds

Generated test file one (100000 lines):
1007089 function calls (1006879 primitive calls) in 0.394 seconds

Generated test file two (1000000 lines):
10011489 function calls (10011279 primitive calls) in 3.962 seconds

Generated test file three (10000000 lines):
100055479 function calls (100055269 primitive calls) in 39.002 seconds

**Median Maintenance - Heap**

All provided test files (including edge cases):
202246 function calls (202203 primitive calls) in 0.074 seconds

Generated test file one (100000 lines):
1540006 function calls (1539963 primitive calls) in 0.542 seconds

Generated test file two (1000000 lines):
15392841 function calls (15392798 primitive calls) in 5.649 seconds

Generated test file three (10000000 lines):
153945805 function calls (153945762 primitive calls) in 59.689 seconds


**QuickSelect**

All provided test files (including edge cases):
145397 function calls (145319 primitive calls) in 0.068 seconds

Generated test file one (100000 lines):
1218115 function calls (1218037 primitive calls) in 0.539 seconds

Generated test file two (1000000 lines):
12854759 function calls (12854681 primitive calls) in 6.208 seconds

Generated test file three (10000000 lines):
119136697 function calls (119136619 primitive calls) in 59.666 seconds


## Reflection

1. What assumptions did you make in your design? Why?

The only assumption I have made in my design is that the user will provide the data in a csv file with an initial line that describes the format of subsequent lines which contain only first names, last names, and ages in some order. My implementation doesn't make any assumption about the order. For instance, the user could create a file with age, lname, fname as the first line, and have each subsequent line contain an age then a last name then a first name.

2. How would you change your program if it had to process many files where each file was over 10M records?

While profiling I noticed that the regular expression matching procedure was responsible for about 30% of the overall runtime. There is significant potential for optimization there, perhaps with simplified parsing or error checking without regex.

3. How would you change your program if it had to process data from more than 10K URLs?

Processing many URLs would involve opening and closing a lot of files, which is an expensive operation. To mitigate this I would combine files initially, either on the command line with cat or with a Python script. Relating to what I discussed in question 2, I may also involve a preprocessing step in which the urls and the lines are validated and combined into files to be processed. This would allow for the program to be modular and make it easier to optimize specific tasks (ie. error checking, url validation, median computation).
